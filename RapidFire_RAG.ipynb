{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2zpTWElkI/qHd0F1U/NIX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamRolander/RAG-Experiments/blob/main/RapidFire_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-First RAG Experimentation Challenge\n",
        "### Custom ArXiv Scientific Literature Corpus with Multi-Query Evaluation\n",
        "\n",
        "**Author:** Adam Rolander\n",
        "**Competition:** RapidFire AI - RAG Track  \n",
        "**Date:** 19 January 2026\n"
      ],
      "metadata": {
        "id": "3RHXZdlFM8TA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Synthesize Dataset"
      ],
      "metadata": {
        "id": "6GjBsMqMjYC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setup & Dependencies\n",
        "\n",
        "This cell installs all required packages and imports necessary libraries for the RAG dataset creation pipeline.\n",
        "\n",
        "**Key Dependencies:**\n",
        "- **Unsloth**: Fast inference for Llama 3.1 model (query generation)\n",
        "- **Scikit-learn**: TF-IDF computation for hard negative mining\n",
        "- **HuggingFace Datasets**: Streaming ArXiv abstract corpus https://huggingface.co/datasets/gfissore/arxiv-abstracts-2021\n",
        "\n",
        "**Reproducibility:** Random seeds are set to ensure consistent sampling across runs.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lM7KKUi_Lj3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes -q\n",
        "!pip install scikit-learn -q\n",
        "\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import defaultdict, Counter\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"‚úì All dependencies installed\")"
      ],
      "metadata": {
        "id": "w6a16mlZCuPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Configuration\n",
        "\n",
        "Define the target corpus structure and evaluation set parameters.\n",
        "\n",
        "**Design Decisions:**\n",
        "- **4 domains** (CS, EESS, Math, Statistics) for balanced STEM coverage\n",
        "- **40,000 documents** with perfect balance (10k per domain)\n",
        "- **1,000 evaluation queries** generated from 500 documents (2 query types each)\n",
        "\n",
        "This configuration ensures:\n",
        "- No domain bias in evaluation metrics\n",
        "- Sufficient scale for robust retrieval testing\n",
        "- Query diversity for testing semantic vs. lexical approaches\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ScdcLvKQL3w9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURATION ---\n",
        "TARGET_DOMAINS = ['cs', 'eess', 'math', 'stat']\n",
        "TOTAL_TARGET = 40000  # 10,000 per domain for robust evaluation\n",
        "PER_DOMAIN_LIMIT = TOTAL_TARGET // len(TARGET_DOMAINS)\n",
        "EVAL_SAMPLE_SIZE = 500  # Will generate 2 queries per doc = 1000 total queries\n",
        "\n",
        "print(f\"Target corpus size: {TOTAL_TARGET:,}\")\n",
        "print(f\"Domains: {', '.join(TARGET_DOMAINS)}\")\n",
        "print(f\"Per-domain limit: {PER_DOMAIN_LIMIT:,}\")\n",
        "print(f\"Eval documents: {EVAL_SAMPLE_SIZE}\")\n",
        "print(f\"Total eval queries: {EVAL_SAMPLE_SIZE * 2}\")\n",
        "print(f\"\\n‚úì Configuration optimized for available data\")"
      ],
      "metadata": {
        "id": "5MK_DCps0MOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Corpus Creation with Balanced Sampling\n",
        "\n",
        "Stream the ArXiv 2021 abstracts dataset and extract exactly 10,000 papers from each target domain.\n",
        "\n",
        "**Sampling Strategy:**\n",
        "- Parse primary category from ArXiv metadata (e.g., `cs.LG` ‚Üí `cs`)\n",
        "- Enforce strict per-domain limits to prevent imbalance\n",
        "- Save to JSONL format for efficient loading\n",
        "\n",
        "**Output:** `working_corpus.jsonl` (40,000 scientific abstracts)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ZCHou8AGL92A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: BALANCED CORPUS CREATION ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"STEP 1: Creating balanced corpus ({PER_DOMAIN_LIMIT:,} per domain)\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "dataset = load_dataset(\"gfissore/arxiv-abstracts-2021\", split=\"train\", streaming=True)\n",
        "counts = defaultdict(int)\n",
        "working_docs = []\n",
        "MIN_ABSTRACT_LENGTH = 20  # Filter out very short/broken abstracts\n",
        "\n",
        "with open(\"working_corpus.jsonl\", \"w\") as f:\n",
        "    for entry in tqdm(dataset, desc=\"Sampling corpus\", total=TOTAL_TARGET):\n",
        "        if len(working_docs) >= TOTAL_TARGET:\n",
        "            break\n",
        "\n",
        "        # Extract primary category prefix (e.g., 'cs' from 'cs.LG')\n",
        "        cats_list = entry[\"categories\"][0].split() if isinstance(entry[\"categories\"][0], str) else entry[\"categories\"]\n",
        "        primary_cat = cats_list[0].split('.')[0].split('-')[0]\n",
        "\n",
        "        # Check domain and count, AND filter by abstract length\n",
        "        abstract_word_count = len(entry[\"abstract\"].split())\n",
        "\n",
        "        if (primary_cat in TARGET_DOMAINS\n",
        "            and counts[primary_cat] < PER_DOMAIN_LIMIT\n",
        "            and abstract_word_count >= MIN_ABSTRACT_LENGTH):\n",
        "\n",
        "            doc = {\n",
        "                \"id\": str(entry[\"id\"]),\n",
        "                \"title\": entry[\"title\"],\n",
        "                \"abstract\": entry[\"abstract\"],\n",
        "                \"categories\": entry[\"categories\"]\n",
        "            }\n",
        "            working_docs.append(doc)\n",
        "            counts[primary_cat] += 1\n",
        "            f.write(json.dumps(doc) + \"\\n\")\n",
        "\n",
        "print(f\"\\n‚úì Corpus created: {len(working_docs):,} documents\")\n",
        "print(f\"‚úì All abstracts have ‚â•{MIN_ABSTRACT_LENGTH} words\")\n",
        "print(f\"‚úì Domain distribution:\")\n",
        "for domain in sorted(TARGET_DOMAINS):\n",
        "    print(f\"  {domain:12s}: {counts[domain]:,}\")\n",
        "\n",
        "with open('checkpoint_corpus.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'working_docs': working_docs,\n",
        "        'counts': dict(counts)\n",
        "    }, f)\n",
        "print(\"‚úì Checkpoint saved: corpus creation complete\")"
      ],
      "metadata": {
        "id": "qXfC7Tpd0mI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Corpus Statistical Analysis\n",
        "\n",
        "Compute comprehensive statistics on the corpus to understand document characteristics and validate data quality.\n",
        "\n",
        "**Metrics Computed:**\n",
        "- Abstract length distribution (mean, std, range)\n",
        "- Title length statistics\n",
        "- Domain balance verification\n",
        "- Sub-category diversity\n",
        "\n",
        "These statistics inform chunking strategy design (e.g., choosing chunk sizes based on average abstract length).\n",
        "\n",
        "**Output:** `corpus_statistics.json`\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "AS2mny8yMCML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 2: COMPUTE CORPUS STATISTICS ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"STEP 2: Computing corpus statistics\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "corpus_df = pd.read_json(\"working_corpus.jsonl\", lines=True)\n",
        "\n",
        "# Compute lengths\n",
        "corpus_df['abstract_length'] = corpus_df['abstract'].str.split().str.len()\n",
        "corpus_df['title_length'] = corpus_df['title'].str.split().str.len()\n",
        "\n",
        "# Extract primary domain for distribution\n",
        "def get_primary_domain(cats):\n",
        "    cats_list = cats[0].split() if isinstance(cats[0], str) else cats\n",
        "    return cats_list[0].split('.')[0].split('-')[0]\n",
        "\n",
        "corpus_df['primary_domain'] = corpus_df['categories'].apply(get_primary_domain)\n",
        "\n",
        "# Compute detailed statistics\n",
        "stats = {\n",
        "    \"total_documents\": len(corpus_df),\n",
        "    \"abstract_length\": {\n",
        "        \"mean\": float(corpus_df['abstract_length'].mean()),\n",
        "        \"std\": float(corpus_df['abstract_length'].std()),\n",
        "        \"min\": int(corpus_df['abstract_length'].min()),\n",
        "        \"max\": int(corpus_df['abstract_length'].max()),\n",
        "        \"median\": float(corpus_df['abstract_length'].median())\n",
        "    },\n",
        "    \"title_length\": {\n",
        "        \"mean\": float(corpus_df['title_length'].mean()),\n",
        "        \"std\": float(corpus_df['title_length'].std()),\n",
        "        \"min\": int(corpus_df['title_length'].min()),\n",
        "        \"max\": int(corpus_df['title_length'].max())\n",
        "    },\n",
        "    \"domain_distribution\": dict(corpus_df['primary_domain'].value_counts().to_dict()),\n",
        "    \"unique_categories\": len(set(cat for cats in corpus_df['categories'] for cat in cats[0].split())),\n",
        "}\n",
        "\n",
        "# Save statistics\n",
        "with open(\"corpus_statistics.json\", \"w\") as f:\n",
        "    json.dump(stats, f, indent=2)\n",
        "\n",
        "# Print summary\n",
        "print(f\"‚úì Total Documents: {stats['total_documents']:,}\")\n",
        "print(f\"\\n  Abstract Length Statistics:\")\n",
        "print(f\"    Mean: {stats['abstract_length']['mean']:.1f} words\")\n",
        "print(f\"    Std:  {stats['abstract_length']['std']:.1f} words\")\n",
        "print(f\"    Range: {stats['abstract_length']['min']}-{stats['abstract_length']['max']} words\")\n",
        "print(f\"    Median: {stats['abstract_length']['median']:.1f} words\")\n",
        "print(f\"\\n  Unique Sub-categories: {stats['unique_categories']}\")\n",
        "print(f\"\\n‚úì Statistics saved to 'corpus_statistics.json'\")"
      ],
      "metadata": {
        "id": "dQ1qOMg60rDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Load Language Model for Query Synthesis\n",
        "\n",
        "Load **Llama 3.1 8B Instruct** in 4-bit quantization for efficient query generation on Colab's free-tier GPU.\n",
        "\n",
        "**Model Choice Rationale:**\n",
        "- Instruction-tuned for following specific prompts\n",
        "- 4-bit quantization enables T4 GPU usage\n",
        "- Strong performance on technical/academic content\n",
        "\n",
        "This model will generate diverse, realistic queries that require semantic understanding to answer correctly.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WhZQpAmVMFzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 3: LOAD MODEL & TOKENIZER ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"STEP 3: Loading LLM for synthetic query generation\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/meta-llama-3.1-8b-instruct-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"‚úì Model loaded successfully\")"
      ],
      "metadata": {
        "id": "3DBXQrrF2bFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Multi-Style Query Generation\n",
        "\n",
        "Generate **2 query types** per document to test retrieval robustness:\n",
        "\n",
        "1. **Technical Queries**: Focus on methodology, approach, and findings\n",
        "2. **Application Queries**: Focus on use cases, implications, and practical impact\n",
        "\n",
        "**Generation Parameters:**\n",
        "- `temperature=0.7` for diversity (avoid repetitive patterns)\n",
        "- `top_p=0.9` for quality control\n",
        "- Validation: Ensure queries end with `?`\n",
        "\n",
        "This dual-query approach tests whether retrieval systems work across different information needs.\n",
        "\n",
        "**Output:** 1,000 queries (500 docs √ó 2 types)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vAQdH3prMIOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 4: GENERATE MULTI-STYLE QUERIES (STRATIFIED SAMPLING) ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"STEP 4: Generating {EVAL_SAMPLE_SIZE * 2} queries (2 styles √ó {EVAL_SAMPLE_SIZE} docs)\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# STRATIFIED SAMPLING: Equal number of docs per domain\n",
        "docs_per_domain = EVAL_SAMPLE_SIZE // len(TARGET_DOMAINS)  # 500 // 4 = 125\n",
        "print(f\"Sampling {docs_per_domain} documents per domain for balanced evaluation\\n\")\n",
        "\n",
        "# Group documents by domain\n",
        "from collections import defaultdict\n",
        "docs_by_domain = defaultdict(list)\n",
        "\n",
        "for doc in working_docs:\n",
        "    cats_list = doc[\"categories\"][0].split() if isinstance(doc[\"categories\"][0], str) else doc[\"categories\"]\n",
        "    primary_cat = cats_list[0].split('.')[0].split('-')[0]\n",
        "    docs_by_domain[primary_cat].append(doc)\n",
        "\n",
        "print(\"\\n‚úì Domain availability check:\")\n",
        "for domain in TARGET_DOMAINS:\n",
        "    available = len(docs_by_domain[domain])\n",
        "    needed = docs_per_domain\n",
        "    status = \"‚úì\" if available >= needed else \"‚úó ERROR\"\n",
        "    print(f\"  {domain:8s}: {available:,} available, {needed} needed {status}\")\n",
        "\n",
        "    if available < needed:\n",
        "        raise ValueError(f\"Domain '{domain}' only has {available} docs, need {needed}!\")\n",
        "\n",
        "print(f\"‚úì All domains have sufficient documents for stratified sampling\\n\")\n",
        "\n",
        "# Sample equally from each domain\n",
        "eval_docs = []\n",
        "for domain in TARGET_DOMAINS:\n",
        "    domain_sample = random.sample(docs_by_domain[domain], docs_per_domain)\n",
        "    eval_docs.extend(domain_sample)\n",
        "    print(f\"  ‚úì Sampled {len(domain_sample)} docs from {domain} domain\")\n",
        "\n",
        "print(f\"\\n‚úì Total eval documents: {len(eval_docs)} (perfectly balanced)\")\n",
        "\n",
        "# Define query styles\n",
        "query_styles = [\n",
        "    {\n",
        "        \"name\": \"technical\",\n",
        "        \"prompt\": \"Write a specific technical question about the methodology, approach, or findings in this abstract. The question should require understanding the abstract to answer.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"application\",\n",
        "        \"prompt\": \"Write a question about the practical applications, implications, or use cases of this research. Focus on how this work could be applied or what problems it solves.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "eval_set = []\n",
        "\n",
        "# Generate queries with progress bar\n",
        "for doc in tqdm(eval_docs, desc=\"Generating queries\"):\n",
        "    for style in query_styles:\n",
        "        # Construct prompt\n",
        "        prompt = (\n",
        "            \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
        "            \"You are a research assistant. Output ONLY a single question.<|eot_id|>\"\n",
        "            \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "            f\"Abstract: {doc['abstract']}\\n\\n\"\n",
        "            f\"Task: {style['prompt']}<|eot_id|>\"\n",
        "            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        )\n",
        "\n",
        "        # Generate\n",
        "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=60,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "        # Extract and clean query\n",
        "        full_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        raw_query = full_output.split(\"assistant\")[-1].strip()\n",
        "        clean_query = raw_query.split('\\n')[0].strip()\n",
        "\n",
        "        # Basic validation: ensure it's a question\n",
        "        if not clean_query.endswith('?'):\n",
        "            clean_query += '?'\n",
        "\n",
        "        # Extract primary category\n",
        "        cats_list = doc[\"categories\"][0].split() if isinstance(doc[\"categories\"][0], str) else doc[\"categories\"]\n",
        "        primary_category = cats_list[0]\n",
        "\n",
        "        eval_set.append({\n",
        "            \"query\": clean_query,\n",
        "            \"query_type\": style[\"name\"],\n",
        "            \"ground_truth_id\": doc[\"id\"],\n",
        "            \"ground_truth_category\": primary_category,\n",
        "            \"ground_truth_title\": doc[\"title\"]\n",
        "        })\n",
        "\n",
        "print(f\"\\n‚úì Generated {len(eval_set)} queries\")\n",
        "print(f\"  Technical queries: {sum(1 for q in eval_set if q['query_type'] == 'technical')}\")\n",
        "print(f\"  Application queries: {sum(1 for q in eval_set if q['query_type'] == 'application')}\")\n",
        "\n",
        "# Check for duplicate queries\n",
        "original_count = len(eval_set)\n",
        "seen_queries = set()\n",
        "deduped_eval_set = []\n",
        "\n",
        "for item in eval_set:\n",
        "    query_lower = item['query'].lower().strip()\n",
        "    if query_lower not in seen_queries:\n",
        "        seen_queries.add(query_lower)\n",
        "        deduped_eval_set.append(item)\n",
        "\n",
        "eval_set = deduped_eval_set\n",
        "duplicates_removed = original_count - len(eval_set)\n",
        "\n",
        "if duplicates_removed > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è Removed {duplicates_removed} duplicate queries\")\n",
        "else:\n",
        "    print(f\"\\n‚úì No duplicate queries detected\")\n",
        "\n",
        "# Verify domain balance in eval set\n",
        "domain_counts = Counter(q['ground_truth_category'].split('.')[0] for q in eval_set)\n",
        "print(f\"\\n‚úì Domain distribution in eval set:\")\n",
        "for domain in TARGET_DOMAINS:\n",
        "    count = domain_counts.get(domain, 0)\n",
        "    print(f\"  {domain:8s}: {count} queries ({100*count/len(eval_set):.1f}%)\")\n",
        "\n",
        "# Sample queries\n",
        "print(f\"\\nSample Technical Query:\")\n",
        "tech_sample = next(q for q in eval_set if q['query_type'] == 'technical')\n",
        "print(f\"  Q: {tech_sample['query']}\")\n",
        "\n",
        "print(f\"\\nSample Application Query:\")\n",
        "app_sample = next(q for q in eval_set if q['query_type'] == 'application')\n",
        "print(f\"  Q: {app_sample['query']}\")\n",
        "\n",
        "# Checkpoint: Save eval set progress\n",
        "with open('checkpoint_eval_set.pkl', 'wb') as f:\n",
        "    pickle.dump(eval_set, f)\n",
        "print(\"‚úì Checkpoint saved: query generation complete\")"
      ],
      "metadata": {
        "id": "oZLrOJJS2ers"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Hard Negative Mining via TF-IDF\n",
        "\n",
        "For each query, identify **5 hard negatives** ‚Äî documents that are semantically similar but do NOT answer the query.\n",
        "\n",
        "**Why Hard Negatives Matter:**\n",
        "- Test whether rerankers can distinguish true positives from near-misses\n",
        "- Prevent retrieval systems from succeeding via simple keyword matching\n",
        "- Enable evaluation of semantic understanding\n",
        "\n",
        "**Method:** TF-IDF cosine similarity to find topically similar but incorrect documents.\n",
        "\n",
        "**Output:** Each query now has `ground_truth_id` + 5 `hard_negative_ids`\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "d173iNGgMOKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 5: GENERATE HARD NEGATIVES ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"STEP 5: Computing hard negatives via TF-IDF similarity\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Prepare corpus texts and IDs\n",
        "corpus_texts = [doc['abstract'] for doc in working_docs]\n",
        "corpus_ids = [doc['id'] for doc in working_docs]\n",
        "id_to_idx = {doc_id: idx for idx, doc_id in enumerate(corpus_ids)}\n",
        "\n",
        "# Compute TF-IDF matrix\n",
        "print(\"Computing TF-IDF vectors...\")\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=2000,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus_texts)\n",
        "\n",
        "print(f\"‚úì TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "\n",
        "# For each eval item, find hard negatives\n",
        "print(\"\\nFinding hard negatives for each query...\")\n",
        "for eval_item in tqdm(eval_set):\n",
        "    # Get ground truth document index\n",
        "    ground_truth_idx = id_to_idx[eval_item['ground_truth_id']]\n",
        "\n",
        "    # Compute query similarity to all documents\n",
        "    query_vec = vectorizer.transform([eval_item['query']])\n",
        "    similarities = cosine_similarity(query_vec, tfidf_matrix)[0]\n",
        "\n",
        "    # Get top similar documents (excluding ground truth)\n",
        "    # We want documents that are similar but NOT the answer\n",
        "    similar_indices = similarities.argsort()[::-1]\n",
        "\n",
        "    # Select top 5 hard negatives (semantically similar but wrong)\n",
        "    hard_negs = []\n",
        "    for idx in similar_indices:\n",
        "        if idx != ground_truth_idx and len(hard_negs) < 5:\n",
        "            hard_negs.append(corpus_ids[idx])\n",
        "\n",
        "    eval_item['hard_negative_ids'] = hard_negs\n",
        "\n",
        "print(f\"\\n‚úì Hard negatives computed for all {len(eval_set)} queries\")\n",
        "\n",
        "# Verify\n",
        "sample = eval_set[0]\n",
        "print(f\"\\nSample Hard Negatives:\")\n",
        "print(f\"  Query: {sample['query'][:80]}...\")\n",
        "print(f\"  Ground Truth: {sample['ground_truth_id']}\")\n",
        "print(f\"  Hard Negatives: {sample['hard_negative_ids'][:3]}\")"
      ],
      "metadata": {
        "id": "HLQIUTX02iwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Query Analysis & Metadata Export\n",
        "\n",
        "Compute statistics on the generated queries to validate quality and diversity.\n",
        "\n",
        "**Metrics:**\n",
        "- Query length distribution\n",
        "- Balance between technical/application queries\n",
        "- Per-domain coverage in evaluation set\n",
        "\n",
        "**Outputs:**\n",
        "- `rag_eval_set.json` (1,000 queries with metadata)\n",
        "- `query_statistics.json` (statistical summary)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "RcowDjQRMSHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 6: COMPUTE QUERY STATISTICS & SAVE ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"STEP 6: Computing query statistics and saving eval set\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Compute query lengths\n",
        "query_lengths = [len(q['query'].split()) for q in eval_set]\n",
        "\n",
        "query_stats = {\n",
        "    \"total_queries\": len(eval_set),\n",
        "    \"queries_per_type\": {\n",
        "        \"technical\": sum(1 for q in eval_set if q['query_type'] == 'technical'),\n",
        "        \"application\": sum(1 for q in eval_set if q['query_type'] == 'application')\n",
        "    },\n",
        "    \"query_length\": {\n",
        "        \"mean\": float(np.mean(query_lengths)),\n",
        "        \"std\": float(np.std(query_lengths)),\n",
        "        \"min\": int(np.min(query_lengths)),\n",
        "        \"max\": int(np.max(query_lengths)),\n",
        "        \"median\": float(np.median(query_lengths))\n",
        "    },\n",
        "    \"queries_per_category\": dict(Counter(q['ground_truth_category'].split('.')[0]\n",
        "                                         for q in eval_set).most_common(10)),\n",
        "    \"hard_negatives_per_query\": 5\n",
        "}\n",
        "\n",
        "# Save eval set\n",
        "with open(\"rag_eval_set.json\", \"w\") as f:\n",
        "    json.dump(eval_set, f, indent=2)\n",
        "\n",
        "# Save query statistics\n",
        "with open(\"query_statistics.json\", \"w\") as f:\n",
        "    json.dump(query_stats, f, indent=2)\n",
        "\n",
        "print(f\"‚úì Eval set saved: {len(eval_set)} queries\")\n",
        "print(f\"\\n  Query Length Statistics:\")\n",
        "print(f\"    Mean: {query_stats['query_length']['mean']:.1f} words\")\n",
        "print(f\"    Std:  {query_stats['query_length']['std']:.1f} words\")\n",
        "print(f\"    Range: {query_stats['query_length']['min']}-{query_stats['query_length']['max']} words\")\n",
        "print(f\"\\n  Top Categories in Eval Set:\")\n",
        "for cat, count in list(query_stats['queries_per_category'].items())[:5]:\n",
        "    print(f\"    {cat}: {count}\")\n",
        "\n",
        "print(f\"\\n‚úì Query statistics saved to 'query_statistics.json'\")"
      ],
      "metadata": {
        "id": "d2o0MiHj2pDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Dataset Validation & Integrity Checks\n",
        "\n",
        "Run comprehensive validation to ensure dataset quality before experimentation.\n",
        "\n",
        "**Validation Checks:**\n",
        "1. Corpus size matches configuration (40,000 docs)\n",
        "2. All ground truth IDs exist in corpus\n",
        "3. All hard negative IDs exist in corpus\n",
        "4. Queries are well-formed (contain `?`)\n",
        "5. Eval set has correct size (1,000 queries)\n",
        "\n",
        "**Expected Result:** All checks should pass with ‚úì\n",
        "\n",
        "If any validation fails, the dataset should be regenerated.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "g9U___UnMUNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 7: VALIDATION & SUMMARY ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"DATASET CREATION COMPLETE - VALIDATION SUMMARY\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Load all files to verify\n",
        "corpus_df = pd.read_json(\"working_corpus.jsonl\", lines=True, dtype={'id': str})\n",
        "eval_df = pd.DataFrame(eval_set)\n",
        "corpus_stats = json.load(open(\"corpus_statistics.json\"))\n",
        "query_stats = json.load(open(\"query_statistics.json\"))\n",
        "\n",
        "# Validation checks\n",
        "print(\"‚úì VALIDATION CHECKS:\")\n",
        "print(f\"  1. Corpus size: {len(corpus_df):,} documents\")\n",
        "print(f\"  2. Eval queries: {len(eval_df):,}\")\n",
        "print(f\"  3. Ground truth coverage: {len(set(eval_df['ground_truth_id']))}/{EVAL_SAMPLE_SIZE} unique docs\")\n",
        "\n",
        "# Check that all ground truth IDs exist in corpus\n",
        "missing_ids = set(eval_df['ground_truth_id']) - set(corpus_df['id'])\n",
        "print(f\"  4. Missing ground truth IDs: {len(missing_ids)} {'‚úó ERROR' if missing_ids else '‚úì'}\")\n",
        "\n",
        "# Check hard negatives exist\n",
        "all_hard_negs = [hn for q in eval_set for hn in q['hard_negative_ids']]\n",
        "missing_hard_negs = set(all_hard_negs) - set(corpus_df['id'])\n",
        "print(f\"  5. Missing hard negative IDs: {len(missing_hard_negs)} {'‚úó ERROR' if missing_hard_negs else '‚úì'}\")\n",
        "\n",
        "# Check query quality\n",
        "queries_with_question_marks = sum(1 for q in eval_set if '?' in q['query'])\n",
        "print(f\"  6. Queries with '?': {queries_with_question_marks}/{len(eval_set)} ({100*queries_with_question_marks/len(eval_set):.1f}%)\")\n",
        "\n",
        "print(f\"\\n‚úì FILES CREATED:\")\n",
        "print(f\"  ‚Ä¢ working_corpus.jsonl        ({len(corpus_df):,} documents)\")\n",
        "print(f\"  ‚Ä¢ rag_eval_set.json           ({len(eval_df):,} queries)\")\n",
        "print(f\"  ‚Ä¢ corpus_statistics.json      (corpus metadata)\")\n",
        "print(f\"  ‚Ä¢ query_statistics.json       (query metadata)\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"SUCCESS! Dataset ready for RAG experimentation\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Print some example queries for manual inspection\n",
        "print(\"EXAMPLE QUERIES FOR MANUAL INSPECTION:\\n\")\n",
        "for i, q in enumerate(eval_df.sample(3).to_dict('records')):\n",
        "    print(f\"{i+1}. [{q['query_type'].upper()}] {q['query']}\")\n",
        "    print(f\"   Ground Truth: {q['ground_truth_id']} ({q['ground_truth_category']})\")\n",
        "    print(f\"   Hard Negatives: {', '.join(q['hard_negative_ids'][:2])}...\\n\")"
      ],
      "metadata": {
        "id": "Gnf6rq3D2plj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Visualization\n",
        "\n",
        "Generate a **4-panel visualization** summarizing the dataset:\n",
        "\n",
        "1. **Domain Distribution**: Verify perfect balance (10k per domain)\n",
        "2. **Abstract Length**: Confirm appropriate range for chunking experiments\n",
        "3. **Query Type Balance**: Verify 50/50 technical/application split\n",
        "4. **Query Length**: Understand query complexity distribution\n",
        "\n",
        "**Output:** `dataset_overview.png` (publication-quality figure)\n",
        "\n",
        "üí° *This visualization is perfect for your competition submission document!*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "eAttifrGMeVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- OPTIONAL: QUICK DATA EXPLORATION ---\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data and RECOMPUTE computed columns\n",
        "corpus_df = pd.read_json(\"working_corpus.jsonl\", lines=True, dtype={'id': str})\n",
        "eval_df = pd.read_json(\"rag_eval_set.json\")\n",
        "corpus_stats = json.load(open(\"corpus_statistics.json\"))\n",
        "query_stats = json.load(open(\"query_statistics.json\"))\n",
        "\n",
        "# Recompute abstract_length (it's not saved in the JSONL)\n",
        "corpus_df['abstract_length'] = corpus_df['abstract'].str.split().str.len()\n",
        "eval_df['query_length'] = eval_df['query'].str.split().str.len()\n",
        "\n",
        "# Extract primary domain\n",
        "def get_primary_domain(cats):\n",
        "    cats_list = cats[0].split() if isinstance(cats[0], str) else cats\n",
        "    return cats_list[0].split('.')[0].split('-')[0]\n",
        "\n",
        "corpus_df['primary_domain'] = corpus_df['categories'].apply(get_primary_domain)\n",
        "\n",
        "# Now proceed with visualization\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Domain distribution in corpus\n",
        "ax1 = axes[0, 0]\n",
        "domain_counts = corpus_df['primary_domain'].value_counts().sort_index()\n",
        "domain_counts.plot(kind='bar', ax=ax1, color='steelblue')\n",
        "ax1.set_title('Domain Distribution in Corpus', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Domain')\n",
        "ax1.set_ylabel('Document Count')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 2. Abstract length distribution\n",
        "ax2 = axes[0, 1]\n",
        "corpus_df['abstract_length'].hist(bins=50, ax=ax2, color='coral', edgecolor='black')\n",
        "ax2.axvline(corpus_stats['abstract_length']['mean'], color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "ax2.set_title('Abstract Length Distribution', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Words')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.legend()\n",
        "\n",
        "# 3. Query type distribution\n",
        "ax3 = axes[1, 0]\n",
        "query_type_counts = pd.Series(query_stats['queries_per_type'])\n",
        "query_type_counts.plot(kind='bar', ax=ax3, color=['#2ecc71', '#e74c3c'])\n",
        "ax3.set_title('Query Type Distribution', fontsize=14, fontweight='bold')\n",
        "ax3.set_xlabel('Query Type')\n",
        "ax3.set_ylabel('Count')\n",
        "ax3.tick_params(axis='x', rotation=0)\n",
        "\n",
        "# 4. Query length distribution\n",
        "ax4 = axes[1, 1]\n",
        "eval_df['query_length'].hist(bins=30, ax=ax4, color='mediumpurple', edgecolor='black')\n",
        "ax4.axvline(query_stats['query_length']['mean'], color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "ax4.set_title('Query Length Distribution', fontsize=14, fontweight='bold')\n",
        "ax4.set_xlabel('Words')\n",
        "ax4.set_ylabel('Frequency')\n",
        "ax4.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('dataset_overview.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Visualization saved as 'dataset_overview.png'\")"
      ],
      "metadata": {
        "id": "4QCG4Fk62tPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backup: Download Dataset Artifacts\n",
        "\n",
        "Package all generated files into a zip archive for local backup and use in the main RAG experimentation notebook.\n",
        "\n",
        "**Files Included:**\n",
        "- `working_corpus.jsonl` (40k documents)\n",
        "- `rag_eval_set.json` (1k queries)\n",
        "- `corpus_statistics.json` (metadata)\n",
        "- `query_statistics.json` (metadata)\n",
        "- `dataset_overview.png` (visualization)\n",
        "\n",
        "**Usage:** Upload this zip to your main RAG notebook to skip dataset generation.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yOYpu5PKMhWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Files you want to persist\n",
        "files_to_save = [\n",
        "    \"working_corpus.jsonl\",\n",
        "    \"rag_eval_set.json\",\n",
        "    \"corpus_statistics.json\",\n",
        "    \"query_statistics.json\",\n",
        "    \"dataset_overview.png\",\n",
        "]\n",
        "\n",
        "zip_name = \"rag_artifacts_backup.zip\"\n",
        "\n",
        "# Create zip\n",
        "with zipfile.ZipFile(zip_name, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
        "    for f in files_to_save:\n",
        "        if os.path.exists(f):\n",
        "            z.write(f)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Missing file: {f}\")\n",
        "\n",
        "# Trigger download\n",
        "files.download(zip_name)"
      ],
      "metadata": {
        "id": "vn_B_HJi2wpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Data"
      ],
      "metadata": {
        "id": "0d4MBS_VKjuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading Cell - Flexible Import System\n",
        "\n",
        "This cell automatically locates and loads the RAG dataset files from multiple possible locations:\n",
        "- Current directory (continuous execution)\n",
        "- `/content` (Colab default upload location)\n",
        "- Google Drive (if mounted)\n",
        "- Local downloads folder\n",
        "\n",
        "**Smart Features:**\n",
        "‚úÖ Searches multiple paths automatically  \n",
        "‚úÖ Validates data integrity after loading  \n",
        "‚úÖ Reports missing files with clear instructions  \n",
        "‚úÖ Verifies ground truth and hard negative coverage\n",
        "\n",
        "**Expected Output:**\n",
        "- `corpus_df`: 40,000 document DataFrame\n",
        "- `eval_df`: 1,000 query DataFrame  \n",
        "- `corpus_stats`: Corpus metadata dictionary\n",
        "- `query_stats`: Query metadata dictionary\n",
        "\n",
        "---\n",
        "\n",
        "üí° **Pro Tip:** This cell can be placed at the start of your RAG experimentation notebook, allowing you to skip the dataset generation steps entirely!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7NLQYOfRMqJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# DATA LOADING CELL - Works for both fresh upload and continuous execution\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LOADING RAG DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define expected files\n",
        "REQUIRED_FILES = {\n",
        "    'corpus': 'working_corpus.jsonl',\n",
        "    'eval_set': 'rag_eval_set.json',\n",
        "    'corpus_stats': 'corpus_statistics.json',\n",
        "    'query_stats': 'query_statistics.json'\n",
        "}\n",
        "\n",
        "# Try to find files in multiple locations\n",
        "def find_file(filename):\n",
        "    \"\"\"Search for file in common Colab locations\"\"\"\n",
        "    search_paths = [\n",
        "        Path('.'),                          # Current directory (if continuous execution)\n",
        "        Path('/content'),                   # Colab default\n",
        "        Path('/content/drive/MyDrive'),     # Google Drive mount\n",
        "        Path.home() / 'Downloads',          # Local downloads\n",
        "    ]\n",
        "\n",
        "    for path in search_paths:\n",
        "        file_path = path / filename\n",
        "        if file_path.exists():\n",
        "            return str(file_path)\n",
        "    return None\n",
        "\n",
        "# Locate all files\n",
        "file_paths = {}\n",
        "missing_files = []\n",
        "\n",
        "for key, filename in REQUIRED_FILES.items():\n",
        "    path = find_file(filename)\n",
        "    if path:\n",
        "        file_paths[key] = path\n",
        "        print(f\"‚úì Found {filename}\")\n",
        "    else:\n",
        "        missing_files.append(filename)\n",
        "        print(f\"‚úó Missing {filename}\")\n",
        "\n",
        "# Handle missing files\n",
        "if missing_files:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"ERROR: Missing required files!\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"\\nPlease upload the following files to your Colab environment:\")\n",
        "    for f in missing_files:\n",
        "        print(f\"  - {f}\")\n",
        "    print(\"\\nYou can drag and drop them into the 'Files' panel on the left.\")\n",
        "    raise FileNotFoundError(f\"Missing files: {missing_files}\")\n",
        "\n",
        "# Load data\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"LOADING DATA INTO MEMORY\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Load corpus\n",
        "print(\"Loading corpus...\")\n",
        "corpus_df = pd.read_json(file_paths['corpus'], lines=True, dtype={'id': str})\n",
        "print(f\"  ‚úì Loaded {len(corpus_df):,} documents\")\n",
        "\n",
        "# Load eval set\n",
        "print(\"Loading evaluation set...\")\n",
        "with open(file_paths['eval_set'], 'r') as f:\n",
        "    eval_set = json.load(f)\n",
        "eval_df = pd.DataFrame(eval_set)\n",
        "print(f\"  ‚úì Loaded {len(eval_df):,} queries\")\n",
        "\n",
        "# Load statistics\n",
        "print(\"Loading statistics...\")\n",
        "with open(file_paths['corpus_stats'], 'r') as f:\n",
        "    corpus_stats = json.load(f)\n",
        "with open(file_paths['query_stats'], 'r') as f:\n",
        "    query_stats = json.load(f)\n",
        "print(f\"  ‚úì Statistics loaded\")\n",
        "\n",
        "# Data validation\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"DATA VALIDATION\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Check corpus integrity\n",
        "assert len(corpus_df) == corpus_stats['total_documents'], \"Corpus size mismatch!\"\n",
        "print(f\"‚úì Corpus size validated: {len(corpus_df):,} documents\")\n",
        "\n",
        "# Check eval set integrity\n",
        "assert len(eval_df) == query_stats['total_queries'], \"Eval set size mismatch!\"\n",
        "print(f\"‚úì Eval set size validated: {len(eval_df):,} queries\")\n",
        "\n",
        "# Check ground truth coverage\n",
        "corpus_ids = set(corpus_df['id'].astype(str))\n",
        "eval_gt_ids = set(eval_df['ground_truth_id'].astype(str))\n",
        "missing_gt = eval_gt_ids - corpus_ids\n",
        "print(f\"‚úì Ground truth coverage: {len(eval_gt_ids - missing_gt)}/{len(eval_gt_ids)} found\")\n",
        "\n",
        "if missing_gt:\n",
        "    print(f\"  ‚ö† Warning: {len(missing_gt)} ground truth IDs not in corpus\")\n",
        "\n",
        "# Check hard negatives coverage\n",
        "all_hard_negs = set()\n",
        "for row in eval_df.itertuples():\n",
        "    all_hard_negs.update(row.hard_negative_ids)\n",
        "missing_hn = all_hard_negs - corpus_ids\n",
        "print(f\"‚úì Hard negatives coverage: {len(all_hard_negs - missing_hn)}/{len(all_hard_negs)} found\")\n",
        "\n",
        "if missing_hn:\n",
        "    print(f\"  ‚ö† Warning: {len(missing_hn)} hard negative IDs not in corpus\")\n",
        "\n",
        "# Display summary statistics\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"DATASET SUMMARY\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "print(\"Corpus:\")\n",
        "print(f\"  Documents: {corpus_stats['total_documents']:,}\")\n",
        "print(f\"  Domains: {list(corpus_stats['domain_distribution'].keys())}\")\n",
        "print(f\"  Avg abstract length: {corpus_stats['abstract_length']['mean']:.1f} words\")\n",
        "print(f\"  Unique categories: {corpus_stats['unique_categories']}\")\n",
        "\n",
        "print(\"\\nEvaluation Set:\")\n",
        "print(f\"  Total queries: {query_stats['total_queries']}\")\n",
        "print(f\"  Technical queries: {query_stats['queries_per_type']['technical']}\")\n",
        "print(f\"  Application queries: {query_stats['queries_per_type']['application']}\")\n",
        "print(f\"  Avg query length: {query_stats['query_length']['mean']:.1f} words\")\n",
        "print(f\"  Hard negatives per query: {query_stats['hard_negatives_per_query']}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"‚úì DATA LOADING COMPLETE - Ready for experimentation!\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Make data easily accessible\n",
        "print(\"Available variables:\")\n",
        "print(\"  - corpus_df: Pandas DataFrame with corpus documents\")\n",
        "print(\"  - eval_df: Pandas DataFrame with evaluation queries\")\n",
        "print(\"  - corpus_stats: Dictionary with corpus statistics\")\n",
        "print(\"  - query_stats: Dictionary with query statistics\")"
      ],
      "metadata": {
        "id": "j5Meeljh3IAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFqmI5WchNWm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}