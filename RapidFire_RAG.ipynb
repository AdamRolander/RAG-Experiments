{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVinbi5FTMdhqmuvNxljsf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamRolander/RAG-Experiments/blob/main/RapidFire_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "6GjBsMqMjYC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AdamRolander/RAG-Experiments.git"
      ],
      "metadata": {
        "id": "V6ElgGL4xjGz",
        "outputId": "28363837-c147-4c64-e6f5-e93fa9770804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RAG-Experiments'...\n",
            "warning: You appear to have cloned an empty repository.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Hr1TDYppVcL",
        "outputId": "4a4a4e31-d05e-4734-d0cb-1b13ac67876d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.6/180.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import torch\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TARGET_DOMAINS = ['cs', 'math', 'stat', 'q-bio', 'q-fin', 'econ', 'eess', 'astro-ph', 'cond-mat', 'quant-ph']\n",
        "TOTAL_TARGET = 50000\n",
        "PER_DOMAIN_LIMIT = TOTAL_TARGET // len(TARGET_DOMAINS)\n",
        "EVAL_SAMPLE_SIZE = 500\n",
        "\n",
        "# --- STEP 1: BALANCED CORPUS CREATION ---\n",
        "print(f\"1. Streaming and sampling a balanced corpus ({PER_DOMAIN_LIMIT} per domain)...\")\n",
        "dataset = load_dataset(\"gfissore/arxiv-abstracts-2021\", split=\"train\", streaming=True)\n",
        "counts = defaultdict(int)\n",
        "working_docs = []\n",
        "\n",
        "with open(\"working_corpus.jsonl\", \"w\") as f:\n",
        "    for entry in dataset:\n",
        "        if len(working_docs) >= TOTAL_TARGET:\n",
        "            break\n",
        "\n",
        "        # Extract primary category prefix (e.g., 'cs' from 'cs.LG')\n",
        "        cats_list = entry[\"categories\"][0].split() if isinstance(entry[\"categories\"][0], str) else entry[\"categories\"]\n",
        "        primary_cat = cats_list[0].split('.')[0].split('-')[0]\n",
        "\n",
        "        if primary_cat in TARGET_DOMAINS and counts[primary_cat] < PER_DOMAIN_LIMIT:\n",
        "            doc = {\n",
        "                \"id\": str(entry[\"id\"]),\n",
        "                \"title\": entry[\"title\"],\n",
        "                \"abstract\": entry[\"abstract\"],\n",
        "                \"categories\": entry[\"categories\"]\n",
        "            }\n",
        "            working_docs.append(doc)\n",
        "            counts[primary_cat] += 1\n",
        "            f.write(json.dumps(doc) + \"\\n\")\n",
        "\n",
        "print(\"Distribution saved:\", dict(counts))\n",
        "\n",
        "# --- STEP 2: LOAD MODEL & TOKENIZER ---\n",
        "print(\"\\n2. Loading local LLM for synthetic query generation...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/meta-llama-3.1-8b-instruct-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# --- STEP 3: GENERATE GOLDEN EVAL SET ---\n",
        "print(f\"\\n3. Generating {EVAL_SAMPLE_SIZE} high-quality queries...\")\n",
        "eval_docs = random.sample(working_docs, EVAL_SAMPLE_SIZE)\n",
        "eval_set = []\n",
        "\n",
        "# Improved prompt to ensure single-sentence, concise questions\n",
        "prompt_style = (\n",
        "    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
        "    \"You are a helpful research assistant. Output ONLY the question text.<|eot_id|>\"\n",
        "    \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "    \"Abstract: {text}\\n\\n\"\n",
        "    \"Task: Write one single, concise research question that is answered by this abstract. \"\n",
        "    \"Do not include any conversational filler.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        ")\n",
        "\n",
        "for doc in eval_docs:\n",
        "    inputs = tokenizer([prompt_style.format(text=doc[\"abstract\"])], return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.7)\n",
        "\n",
        "    # Extract only the generated portion\n",
        "    full_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    raw_query = full_output.split(\"assistant\")[-1].strip()\n",
        "    clean_query = raw_query.split('\\n')[0].strip() # Take only the first line to avoid LLM \"chatter\"\n",
        "\n",
        "    eval_set.append({\n",
        "        \"query\": clean_query,\n",
        "        \"ground_truth_id\": doc[\"id\"]\n",
        "    })\n",
        "\n",
        "with open(\"rag_eval_set.json\", \"w\") as f:\n",
        "    json.dump(eval_set, f, indent=4)\n",
        "\n",
        "print(\"\\nSuccess! Files 'working_corpus.jsonl' and 'rag_eval_set.json' are ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN2PWTvzEGHa",
        "outputId": "302a7785-f3f3-4ae2-e996-3cdd85783cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Streaming and sampling a balanced corpus (1000 per domain)...\n",
            "Distribution saved: {'math': 1000, 'cs': 1000, 'stat': 1000, 'eess': 1000, 'econ': 1000}\n",
            "\n",
            "2. Loading local LLM for synthetic query generation...\n",
            "==((====))==  Unsloth 2025.12.8: Fast Llama patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: af7c48ad-1b5b-4478-b895-a04e81bbb4c9)')' thrown while requesting HEAD https://huggingface.co/unslothai/colab/resolve/20f9daee9da18936efa03ad4e1361884c60cca0c/model.safetensors\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: af7c48ad-1b5b-4478-b895-a04e81bbb4c9)')' thrown while requesting HEAD https://huggingface.co/unslothai/colab/resolve/20f9daee9da18936efa03ad4e1361884c60cca0c/model.safetensors\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. Generating 500 high-quality queries...\n",
            "\n",
            "Success! Files 'working_corpus.jsonl' and 'rag_eval_set.json' are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_df = pd.read_json(\"working_corpus.jsonl\", lines=True)\n",
        "eval_df = pd.read_json(\"rag_eval_set.json\")\n",
        "\n",
        "print(f\"Corpus Categories found: {corpus_df['categories'].iloc[0]}\")\n",
        "missing = eval_df[~eval_df['ground_truth_id'].astype(str).isin(corpus_df['id'].astype(str))]\n",
        "print(f\"Missing IDs count: {len(missing)}\")\n",
        "\n",
        "# Sample clean query check\n",
        "print(f\"Sample Query: {eval_df['query'].iloc[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4zDBKA2tmcJ",
        "outputId": "40707836-6d4c-4fdf-92ea-4a9c0194af5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus Categories found: ['math.CO cs.CG']\n",
            "Missing IDs count: 0\n",
            "Sample Query: What is the vulnerability of the fuzzy vault approach when implemented with fingerprint data?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Load the corpus\n",
        "corpus_df = pd.read_json(\"working_corpus.jsonl\", lines=True)\n",
        "\n",
        "# 2. Extract and flatten categories\n",
        "# ArXiv categories are usually space-separated strings (e.g., \"cs.LG cs.AI\")\n",
        "def flatten_categories(cat_entry):\n",
        "    if isinstance(cat_entry, list):\n",
        "        return [c for item in cat_entry for c in item.split()]\n",
        "    return cat_entry.split()\n",
        "\n",
        "all_cats = []\n",
        "for entry in corpus_df['categories']:\n",
        "    all_cats.extend(flatten_categories(entry))\n",
        "\n",
        "# 3. Calculate and Print Top 5\n",
        "top_5 = Counter(all_cats).most_common(5)\n",
        "\n",
        "print(\"=\" * 35)\n",
        "print(f\"{'Category':<15} | {'Count':<10}\")\n",
        "print(\"-\" * 35)\n",
        "for cat, count in top_5:\n",
        "    print(f\"{cat:<15} | {count:<10}\")\n",
        "print(\"=\" * 35)\n",
        "print(f\"Total Unique Categories: {len(Counter(all_cats))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB4Egja4B7xE",
        "outputId": "7749b5cb-678c-4b06-ebd5-2d073014f544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================\n",
            "Category        | Count     \n",
            "-----------------------------------\n",
            "eess.SP         | 761       \n",
            "stat.ME         | 633       \n",
            "econ.EM         | 555       \n",
            "stat.AP         | 484       \n",
            "cs.IT           | 398       \n",
            "===================================\n",
            "Total Unique Categories: 132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w6a16mlZCuPX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}